{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "scGMVAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiangyuan2li/scDGM/blob/master/GMVAE/scGMVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iODYL0Nh20TI",
        "colab_type": "text"
      },
      "source": [
        "## scGMVAE\n",
        "This framework is modified from scVI with a Gaussian mixture latent variable variantion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEqc-nd120TK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scanpy\n",
        "# !pip3 install pyro-ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWNJcCQT20TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBoklrrp20TQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scanpy as sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNa7NIGR20TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import anndata\n",
        "from torch.distributions import Categorical, Normal, kl_divergence as kl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzDMc4Ow20TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
        "from sklearn.metrics import adjusted_rand_score as ARI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXppF5Te20Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Ery_p120Tc",
        "colab_type": "text"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCC2x2q20Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AnnDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X_ann):\n",
        "        # TODO\n",
        "        # 1. Initialize file paths or a list of file names.\n",
        "        self.X = X_ann.obsm['X']\n",
        "        self.local_mean = X_ann.obs['local_mean']\n",
        "        self.local_var = X_ann.obs['local_var']\n",
        "#        self.batch_index = X_ann.obs['batch_index']\n",
        "#        self.fake_labels = X_ann.obs['fake_labels']\n",
        "    def __getitem__(self, index):\n",
        "        # TODO\n",
        "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
        "        sample_x = self.X[index]\n",
        "        sample_local_mean = self.local_mean[index]\n",
        "        sample_local_var = self.local_var[index]\n",
        "#        sample_batch_index = self.batch_index[index]\n",
        "#        sample_fake_labels = self.fake_labels[index]\n",
        "\n",
        "        sample = {'x': sample_x, 'local_mean':sample_local_mean,'local_var':sample_local_var}\n",
        "#                 'batch_index':sample_batch_index,'fake_labels':sample_fake_labels}\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        # the total size of your dataset.\n",
        "        return len(self.X)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8h7m99820Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_adata = sc.read_h5ad(\"cortex_scAnnData.h5ad\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-FpWUIn20Tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_adata.obsm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfHSCyp620Tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_adata.obsm['X']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm-1OWVV20Tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_mean = np.log(raw_adata.obsm['X'].sum(axis=1)).mean()\n",
        "\n",
        "local_var = np.log(raw_adata.obsm['X'].sum(axis=1)).var()\n",
        "\n",
        "raw_adata.obs['local_mean'] = local_mean\n",
        "\n",
        "raw_adata.obs['local_var'] = local_var\n",
        "\n",
        "#raw_adata.obs['clusters'] = raw_adata.obs['Group']\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "ann_dataset = AnnDataset(raw_adata)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=ann_dataset,\n",
        "                                           batch_size=BATCH_SIZE, \n",
        "                                           shuffle=True,drop_last = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqDvYI2b20To",
        "colab_type": "text"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJK0Uvns20Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_zinb_positive(x, mu, theta, pi, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Variables:\n",
        "    mu: mean of the negative binomial (has to be positive support) (shape: minibatch x genes)\n",
        "    theta: inverse dispersion parameter (has to be positive support) (shape: minibatch x genes)\n",
        "    pi: logit of the dropout parameter (real support) (shape: minibatch x genes)\n",
        "    eps: numerical stability constant\n",
        "    \"\"\"\n",
        "\n",
        "    # theta is the dispersion rate. If .ndimension() == 1, it is shared for all cells (regardless of batch or labels)\n",
        "    if theta.ndimension() == 1:\n",
        "        theta = theta.view(\n",
        "            1, theta.size(0)\n",
        "        )  # In this case, we reshape theta for broadcasting\n",
        "\n",
        "    softplus_pi = F.softplus(-pi)\n",
        "    #print(\"softplus_pi\", softplus_pi)\n",
        "    log_theta_eps = torch.log(theta + eps)\n",
        "    #print(\"log_theta_eps\", log_theta_eps)\n",
        "    log_theta_mu_eps = torch.log(theta + mu + eps)\n",
        "    #print(\"log_theta_mu_eps\", log_theta_mu_eps)\n",
        "    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)\n",
        "    #print(\"pi_theta_log\", pi_theta_log)\n",
        "\n",
        "    case_zero = F.softplus(pi_theta_log) - softplus_pi\n",
        "    #print(\"case_zero\", case_zero)\n",
        "    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)\n",
        "    #print(\"mul_case_zero\", mul_case_zero)\n",
        "\n",
        "    case_non_zero = (\n",
        "        -softplus_pi\n",
        "        + pi_theta_log\n",
        "        + x * (torch.log(mu + eps) - log_theta_mu_eps)\n",
        "        + torch.lgamma(x + theta)\n",
        "        - torch.lgamma(theta)\n",
        "        - torch.lgamma(x + 1)\n",
        "    )\n",
        "    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)\n",
        "\n",
        "    res = mul_case_zero + mul_case_non_zero\n",
        "\n",
        "    return res\n",
        "  \n",
        "def log_normal(x, mu, var, eps = 0.0, dim = 0):\n",
        "    if eps > 0.0:\n",
        "        var = var + eps\n",
        "    return torch.sum(-1/2 * (torch.log(2 * torch.tensor(np.pi)) + torch.log(var) + (x - mu)**2 / var), dim = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cypNcKQq20Tt",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlYvb5ww6Gzc",
        "colab_type": "text"
      },
      "source": [
        "## Q(Y|X) Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkyoqYKP20Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class qy_given_x_encoder(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden, n_clusters):\n",
        "        super(qy_given_x_encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(n_in, n_hidden),\n",
        "            nn.BatchNorm1d(n_hidden, momentum=0.01, eps=0.001),\n",
        "            nn.ReLU(),\n",
        "          )\n",
        "        \n",
        "        self.logits = nn.Sequential(\n",
        "            nn.Linear(n_hidden, n_clusters),\n",
        "            nn.BatchNorm1d(n_clusters),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        logit = self.logits(x)\n",
        "\n",
        "        q_y_given_x = Categorical(logits=logit)\n",
        "\n",
        "        return q_y_given_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN4QLFCKQKQU",
        "colab_type": "text"
      },
      "source": [
        "## Q(Z|X,Y) Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1DhldmKQJUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class qz_given_xy_encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 n_in, \n",
        "                 n_clusters,\n",
        "                 n_hidden, \n",
        "                 n_out, \n",
        "                 n_iw_samples, \n",
        "                 n_mc_samples, \n",
        "                 latent_size):\n",
        "      \n",
        "      super(qz_given_xy_encoder, self).__init__()\n",
        "      self.n_clusters = n_clusters\n",
        "      self.n_iw_samples = n_iw_samples\n",
        "      self.n_mc_samples = n_mc_samples\n",
        "      self.latent_size = latent_size\n",
        "\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Linear(n_in + n_clusters, n_hidden),\n",
        "          nn.BatchNorm1d(n_hidden),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mean_encoder = nn.Linear(n_hidden, n_out)\n",
        "      self.var_encoder = nn.Sequential(\n",
        "          nn.Linear(n_hidden, n_out),\n",
        "          nn.Softplus()\n",
        "      )\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, y):\n",
        "      y = y.unsqueeze(0).repeat(x.size(0),1)\n",
        "      cat = torch.cat([x,y], dim=1)\n",
        "      q = self.encoder(cat)\n",
        "      \n",
        "      q_m = self.mean_encoder(q)\n",
        "      q_v = self.var_encoder(q)\n",
        "\n",
        "      q_z_given_x_y = Normal(q_m, torch.sqrt(q_v))\n",
        "\n",
        "      z_mean = q_z_given_x_y.mean\n",
        "\n",
        "      z = q_z_given_x_y.rsample(torch.Size([\n",
        "                                  self.n_iw_samples * self.n_mc_samples\n",
        "                                ]))\n",
        "\n",
        "      return q_z_given_x_y, z_mean, z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWZS8lKGzW50",
        "colab_type": "text"
      },
      "source": [
        "## P(Z|Y) Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nroGNVIizdeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pz_given_y_encoder(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden, n_out):\n",
        "        super(pz_given_y_encoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(n_in, n_hidden),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mean_encoder = nn.Linear(n_hidden, n_out)\n",
        "        self.var_encoder = nn.Sequential(\n",
        "            nn.Linear(n_hidden, n_out),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        \n",
        "    def forward(self, y):\n",
        "        q = self.encoder(y)\n",
        "        \n",
        "        p_m = self.mean_encoder(q)\n",
        "        p_v = self.var_encoder(q)\n",
        "\n",
        "        p_z_given_y = Normal(p_m, torch.sqrt(p_v))\n",
        "\n",
        "        z_mean = p_z_given_y.mean\n",
        "        \n",
        "        return p_z_given_y, z_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DViGpZR7BcD",
        "colab_type": "text"
      },
      "source": [
        "## P(X|Z) Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwgok1dS20Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class px_given_z_decoder(nn.Module):\n",
        "    def __init__(self, n_in, n_clusters, n_hidden, n_out):\n",
        "        super(px_given_z_decoder, self).__init__()\n",
        "        self.px_decoder = nn.Sequential(\n",
        "            nn.Linear(n_in, n_hidden),\n",
        "            nn.BatchNorm1d(n_hidden, momentum=0.01, eps=0.001),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.decoder_pi = nn.Sequential(\n",
        "            nn.Linear(n_hidden, n_out),\n",
        "            nn.BatchNorm1d(n_out),\n",
        "            nn.Softplus(),\n",
        "        )\n",
        "\n",
        "        self.decoder_p = nn.Sequential(\n",
        "            nn.Linear(n_hidden, n_out),\n",
        "            nn.BatchNorm1d(n_out),\n",
        "            nn.Softplus(),\n",
        "        )\n",
        "\n",
        "        self.decoder_log_r = nn.Sequential(\n",
        "            nn.Linear(n_hidden, n_out),\n",
        "        )\n",
        "        \n",
        "    def forward(self, z: torch.Tensor):\n",
        "        z = self.px_decoder(z)\n",
        "        \n",
        "        pi = self.decoder_pi(z)\n",
        "        p = self.decoder_p(z)\n",
        "        log_r = self.decoder_log_r(z)\n",
        "\n",
        "        return pi, p, log_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62rz0NC97IEn",
        "colab_type": "text"
      },
      "source": [
        "## GMVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1PcBFt20T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GMVAE(nn.Module):\n",
        "    def __init__(self, \n",
        "                 n_input, \n",
        "                 device,\n",
        "                 n_hidden=128, \n",
        "                 latent_size=32, \n",
        "                 n_clusters=7, \n",
        "                 n_iw_samples=1, \n",
        "                 n_mc_samples=1, \n",
        "                 kl_weight=1,\n",
        "                 warm_up_weight=1):\n",
        "        super(GMVAE, self).__init__()\n",
        "        self.device = device\n",
        "        self.latent_size = latent_size\n",
        "        self.n_iw_samples = n_iw_samples\n",
        "        self.n_mc_samples = n_mc_samples\n",
        "        self.n_clusters = n_clusters\n",
        "        self.warm_up_weight = warm_up_weight\n",
        "        self.kl_weight = kl_weight\n",
        "        self.recon_theta = nn.Parameter(\n",
        "              torch.abs(\n",
        "                  torch.randn(n_input)\n",
        "              ).clamp(0,100000)\n",
        "            ) \n",
        "\n",
        "        self.q_y_x_encoder = qy_given_x_encoder(n_input, n_hidden, n_clusters)\n",
        "        self.q_z_xy_encoder = nn.ModuleList([])\n",
        "        self.p_z_y_encoder = nn.ModuleList([])\n",
        "        self.p_x_z_decoder = nn.ModuleList([])\n",
        "\n",
        "        for k in range(n_clusters):\n",
        "            self.q_z_xy_encoder.append(qz_given_xy_encoder(\n",
        "                n_input, \n",
        "                n_clusters, \n",
        "                n_hidden, \n",
        "                self.latent_size, \n",
        "                self.n_iw_samples, \n",
        "                self.n_mc_samples, \n",
        "                self.latent_size\n",
        "              ))\n",
        "            self.p_z_y_encoder.append(\n",
        "                pz_given_y_encoder(n_clusters, n_hidden, self.latent_size)\n",
        "            )\n",
        "            self.p_x_z_decoder.append(\n",
        "                px_given_z_decoder(self.latent_size, n_clusters, n_hidden, n_input)\n",
        "            )\n",
        "\n",
        "\n",
        "        self.q_z_xy = [None] * self.n_clusters\n",
        "        self.p_z_y = [None] * self.n_clusters\n",
        "        self.z = [None] * self.n_clusters\n",
        "        kl_divergence_z_mean = [None] * self.n_clusters\n",
        "        log_p_x_given_z_mean = [None] * self.n_clusters\n",
        "        p_x_means = [None] * self.n_clusters\n",
        "        mean_of_p_x_given_z_variances = [None] * self.n_clusters\n",
        "        variance_of_p_x_given_z_means = [None] * self.n_clusters\n",
        "        self.p_z_mean = [None] * self.n_clusters\n",
        "        self.p_x_z = [None] * self.n_clusters\n",
        "        self.p_z_means = []\n",
        "        self.p_z_variances = []\n",
        "        self.q_z_means = []\n",
        "        self.q_z_variances = []\n",
        "\n",
        "        # p(y)\n",
        "        p_y_probabilities = torch.ones(self.n_clusters)/self.n_clusters\n",
        "        p_y_logits = torch.log(p_y_probabilities).to(device)\n",
        "        self.p_y = Categorical(logits=p_y_logits)\n",
        "        self.p_y_probabilities = self.p_y.probs.unsqueeze(0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # Y latent space\n",
        "        # p(y)\n",
        "        p_y_samples = self.p_y.sample(sample_shape=torch.Size([x.size(0),1]))\n",
        "        self.p_y_samples = torch.zeros([x.size(0), self.n_clusters]).to(self.device)\n",
        "        for row in range(len(self.p_y_samples)):\n",
        "          self.p_y_samples[row][p_y_samples[row]] = 1\n",
        "\n",
        "        # q(y|x)\n",
        "        y = torch.eye(self.n_clusters).to(self.device)\n",
        "        self.q_y_x = self.q_y_x_encoder(x)\n",
        "        self.q_y_logits = torch.log(self.q_y_x.probs) - torch.log1p(-self.q_y_x.probs)\n",
        "        self.q_y_probabilities = torch.mean(self.q_y_x.probs, dim=0)\n",
        "\n",
        "\n",
        "      # Z Latent Space\n",
        "        z_mean = [None]*self.n_clusters\n",
        "        for k in range(self.n_clusters):\n",
        "          # q(z|x,y)\n",
        "          self.q_z_xy[k], z_mean[k], self.z[k] = self.q_z_xy_encoder[k](x, y[k])\n",
        "          \n",
        "          #p(z|y)\n",
        "          self.p_z_y[k], self.p_z_mean[k] = self.p_z_y_encoder[k](y[k].unsqueeze(0))\n",
        "\n",
        "        self.y = self.q_y_x.probs\n",
        "        \n",
        "      # Decoder X\n",
        "        pi = [None] * self.n_clusters\n",
        "        p = [None] * self.n_clusters\n",
        "        log_r = [None] * self.n_clusters\n",
        "        for k in range(self.n_clusters):\n",
        "          # p(x|z)\n",
        "          pi[k], p[k], log_r[k] = self.p_x_z_decoder[k](self.z[k].squeeze())\n",
        "\n",
        "      # Loss\n",
        "        kl_divergence_y = kl(self.q_y_x, self.p_y).mean()\n",
        "\n",
        "        z_reshaped = [\n",
        "            torch.reshape(\n",
        "                self.z[k],\n",
        "                shape=[\n",
        "                    self.n_iw_samples,\n",
        "                    self.n_mc_samples,\n",
        "                    -1,\n",
        "                    self.latent_size\n",
        "                ]\n",
        "            )\n",
        "            for k in range(self.n_clusters)\n",
        "        ]\n",
        "        \n",
        "        kl_divergence_z_mean = torch.zeros(x.size(0)).to(self.device)\n",
        "        reconstruct_losses = torch.zeros(self.n_clusters, x.size(0)).to(self.device)\n",
        "        for k in range(self.n_clusters):\n",
        "          log_q_z_given_x_y = torch.sum(\n",
        "              self.q_z_xy[k].log_prob(\n",
        "                  z_reshaped[k]\n",
        "              ),\n",
        "              -1\n",
        "          )\n",
        "\n",
        "          log_p_z_given_y = torch.sum(\n",
        "              self.p_z_y[k].log_prob(\n",
        "                  z_reshaped[k]\n",
        "              ),\n",
        "              -1\n",
        "          )\n",
        "\n",
        "          kl_divergence_z_mean += torch.mean(\n",
        "              log_q_z_given_x_y - log_p_z_given_y\n",
        "          ) * self.y[:,k]\n",
        "\n",
        "          reconstruct_losses[k] = -log_zinb_positive(x, pi[k], p[k], log_r[k]).sum(dim = 1)\n",
        "\n",
        "        self.pi = pi\n",
        "        self.p = p\n",
        "        self.log_r = log_r        \n",
        "\n",
        "        self.kl_divergence_z = torch.mean(kl_divergence_z_mean)\n",
        "        self.kl_divergence_y = torch.mean(kl_divergence_y)\n",
        "        self.reconstruction_error = torch.mean(reconstruct_losses)\n",
        "\n",
        "        self.lower_bound_weighted = (\n",
        "            self.reconstruction_error\n",
        "            + self.warm_up_weight * self.kl_weight * (\n",
        "                self.kl_divergence_z + kl_divergence_y\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return self.lower_bound_weighted, self.kl_divergence_z, self.kl_divergence_y, self.reconstruction_error\n",
        "    \n",
        "    def get_latent_cluster(self, x):\n",
        "      with torch.no_grad():\n",
        "        y = torch.eye(self.n_clusters).to(self.device)\n",
        "        latent = torch.zeros(self.n_clusters, x.size(0),self.latent_size)\n",
        "        for k in range(self.n_clusters):\n",
        "          self.q_z_xy_encoder[k].eval()\n",
        "          _, _, z = self.q_z_xy_encoder[k](x, y[k])\n",
        "          latent[k] = z.squeeze()\n",
        "          self.q_z_xy_encoder[k].train()\n",
        "\n",
        "      return latent.permute(1,0,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XSZJkNU20T7",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjm-XAkE20T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "gmvae = GMVAE(n_input=raw_adata.obsm['X'].shape[1], \n",
        "              device=device,\n",
        "              n_hidden=128, \n",
        "              latent_size=32, \n",
        "              n_clusters=7,\n",
        "              kl_weight=0.5,\n",
        "              ).to(device)\n",
        "\n",
        "learning_rate=1e-2\n",
        "weight_decay = 1e-6\n",
        "eps = 0.01\n",
        "epoch = 0\n",
        "params = gmvae.parameters()\n",
        "optimizer = torch.optim.AdamW(\n",
        "            params, lr=learning_rate, weight_decay=weight_decay\n",
        "        )   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1DgI87220UB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader,optimizer, device, val_loader = None, warm_up_epoch = 20,num_epochs=30, seed=1,\n",
        "          save_path = \"train_with_tsne/\"):\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    model.warm_up_weight = min(epoch / warm_up_epoch, 1.0)\n",
        "\n",
        "    for i, sample in enumerate(train_loader):\n",
        "      x = sample['x'].to(device=device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss, kl_divergence_z, kl_divergence_y, reconstruction_error = model(x)\n",
        "      loss.backward()\n",
        "      param1 = optimizer.param_groups[0]\n",
        "      nn.utils.clip_grad_norm(model.parameters(), 5)\n",
        "      #print(param1)\n",
        "      optimizer.step()\n",
        "      param2 = optimizer.param_groups[0]\n",
        "      #print(param2)\n",
        "\n",
        "      if i % 5 == 0:\n",
        "          print(\"Epoch[{}/{}], Step [{}/{}],  Loss: {:.4f}, KL Div Z: {:.4f}, KL Div Y: {:.4f}, Recon Loss: {:.4f}\".format(\n",
        "                                  epoch, num_epochs, i, len(train_loader), loss.item(), kl_divergence_z.item(), kl_divergence_y.item(), reconstruction_error.item()))\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAf-Pj4120UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(gmvae,train_loader,optimizer, device=device, num_epochs = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcFntUPdIF3I",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZJmMwWG20UM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent = gmvae.get_latent_cluster(torch.Tensor(raw_adata.obsm['X']).cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH6m_ZAsjk8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEJtHRmO20UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent = np.argmax(latent,axis=1)\n",
        "latent.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3RfS6TSEcQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_adata = sc.AnnData(X=raw_adata.X)\n",
        "post_adata.obsm[\"X_dca\"] = latent.cpu().detach().numpy()\n",
        "post_adata.obs['cell_type'] = raw_adata.obs['clusters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtmkYO28Erzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.pp.neighbors(post_adata, use_rep=\"X_dca\", n_neighbors=15)\n",
        "sc.tl.umap(post_adata, min_dist=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oimWZdqEuJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZh1imBREv4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sc.pl.umap(post_adata, color=[\"cell_type\"], ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y9RrBvdOhGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_adata.obs['clusters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAOkq01420UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ARI(latent,raw_adata.obs['clusters'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jmjMnte20UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NMI(l,raw_adata.obs['clusters'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zHSSZGW20Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_adata = anndata.AnnData(X=raw_adata.X)\n",
        "post_adata.obsm[\"X_scVI\"] = latent.detach().numpy()\n",
        "post_adata.obs['cell_type'] = raw_adata.obs['clusters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP1SMZqT20Uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.pp.neighbors(post_adata, use_rep=\"X_scVI\", n_neighbors=15)\n",
        "sc.tl.umap(post_adata, min_dist=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iml1_Ipd20Uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sc.pl.umap(post_adata, color=[\"cell_type\"], ax=ax)#, show=show_plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W24YopBR20Uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.tl.tsne(post_adata, use_rep=\"X_scVI\")\n",
        "sc.pl.tsne(post_adata,color=['cell_type'])#,save=\"_cortex_scvi.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq5zqmM020Ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "out = KMeans(n_clusters=7).fit(latent)\n",
        "\n",
        "out.labels_\n",
        "\n",
        "sklearn.metrics.normalized_mutual_info_score(out.labels_,raw_adata.obs['clusters'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGyzrRDK20Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sklearn.metrics.adjusted_rand_score(out.labels_,raw_adata.obs['clusters'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}